---
title: 'Energy Based Models for AI (Part 1) : What does Energy stands for?'
date: 2025-09-27T00:00:00.000Z
log_type: reading
parent_stream: src/content/research-streams/energy-based-lnn.mdx
tags:
  - energy-based-models
  - yann-lecun
  - boltzmann-machine
summary: The first article discussing Energy Based Models
---

![](/Screenshot%202025-11-23%20at%208.02.41%E2%80%AFAM.png)

In the last few months, in July actually, several researchers from Amazon, Stanford and Harvard University published a paper discussing on Energy Based Transformers. You may read the paper here: https\://arxiv.org/pdf/2507.02092

The idea behind this paper is to show that Energy Based Transformer are better at scaling and better at thinking (compared to reasoning method that we use currently). I don't want to go into detail what the paper is all about, but it does trigger thousands of questions in my brain.

One of the main one, "What does the term energy doing there?"

Coming from energy industry, I was severely confused. Especially when reading the word energy based transformer. Isn't transformer is already energy based?

But the energy here actually refers to a different paradigm in how we devise and train our machine learning/neural network model. So today, and for the next few months, we will explore the concept of energy based model and understand in details on what it's all about.

### Where does this comes from?

Based on my reading, the first time the concept of energy comes from, is from Hopfield Network, published in 1982. John Hopfield who won the nobel prize for this network, introduce a neural network that trains with the objective to minimize the energy.

Geoffrey Hinton and his friend which is hard for me to spell his name here,improved upon that model, and created the Boltzmann Machine in 1985. Which extended the idea by imagine the model as randomized probability distribution.

Then in 2006, Yann LeCunn and his colleagues generalized this model into a complete framework and published it. You can read it here, and it serves as introductory for anyone who intends to delve into this. You can read it here: https\://web.stanford.edu/class/cs379c/archive/2012/suggested\_reading\_list/documents/LeCunetal06.pdf

This will be my guide in writing this series for the months to come

### What do I think?

So, for this part, this is based on my opinion really. Haven't really done a complete structured research on this, but this is what I feel based on what I gleamed on the current AI research trend.

We know that since Transformer comes out in 2017, we have been having a hard time to understand what our neural networks are doing. Some of the researchers even point out that the point began in 2011, with the existence of Alex Net. The model has becomes too big, and we don't really understand the inner workings of the model.

We know how to make, we know what the operation needed, component needed, but we don't why the model behaves that way during and after training. Let me give you a more concrete example.

Say we train a transformer for text prediction. We know that once we finished training, the key, query and value of our data is not mapped at the higher dimensions. We know that will be the results. What we don't know is, where the map location would be. And the results will be different across different transformer, even different training runs.

So, the idea behind energy, where we model the probability problem in the forms of energy try to bring that challenges with a solution. If we can model the probability problems/distribution problem as a physics problem, we can use phyiscs to control and solve it. And with this, we can gain a new way to control the internal dynamicas of our model.

That is why, I think the reason why this model has been gaining traction recently.

### So, what is energy in terms of probabilities?

We know that when we train a neural network or a machine learning model, we always gives X (our input) to find Y. Sometimes, we gives one X, and from there we try to predict Y. Sometimes we have multiple X to predict one Y. and sometimes we have multiple X and multiple Y. All this are done discretely, separately without thinking of the latent connection between these variables.

Then comes the generative model, that try to find the latent connection between these variables to ensure that all dependencies are encoded. But since this encoding is done during the training, and across billions of parameters, we cannot fully tracks where this encoding happens, thus results in the "black box" that we have know (although I don't agree with the term, I understand now where it comes from)

So, this is where energy comes from. We model early one these connection between each variables through usage of a scalar energy. Then during training, we will minimize the energy of the correct connection (or configuration), and maximize the energy of wrong connection. By doing so, we will create an energy landscape for our data probabilities, that will be easily manouvered and experimented on when we need it later. With this energy landscape establish, we can use physics principle to navigate this landscape and try to understand fully our model. This is better than the "black box" approach that we currently have.

So in mathematical sense, instead of normal

ùëå‚àó=ùêπ(ùëã) 

where  ùëå‚àó  is predicted Y and  ùêπ(ùëã)  is the statistic model/ machine learning model that we use. We now got

ùëå‚àó=argminùëå‚ààyùê∏(ùëå,ùëã) 

where given X, we try to find the best Y where it produces the lowest energy score. To put it more simply, we gives a compatibility score for each pair of X and Y. The one that is more correct will be selected based on the lowest energy that is produced.

Now got it?

I know that you guys have started scratching your head, so let's do some demonstration to show how this works

### Let's make a simple neural network

Let's make a simple neural network. And let's train and plot the energy function after training. The model would be just a simple 2 layers with activation function in between. Simple ReLU for the first one and Tanh for the final activation function. Why Tanh? Because if just using ReLu, it will create exploding gradient at the end. So we use Tanh.

We define the correct point, the correct X and Y combination, which is point \[1.0,1.0], and we will try to train the model to minimize the energy of this point, and maximize the energy of other wrong points.

And we will plot and look at this energy landscape that we have created. So let's do that

<CodeBlock
  code="#first download the package and library
  import torch
  import torch.nn as nn
  import torch.optim as optim
  import numpy as np
  import matplotlib.pyplot as plt


  #we define our correct point in the x,y space
  correct_point = torch.tensor([[1.0,1.0]])

  #then we define our model
  class EnergyModel(nn.Module):
    def __init__(self):

      super().__init__()
      self.model = nn.Sequential(
          nn.Linear(2,64),
          nn.ReLU(),
          nn.Linear(64,1),
          nn.Tanh() #have to use Tanh to prevent exploding gradient
      )

    def forward(self,x):
      return self.model(x)

  #let's make function to plot our energy surface in 2d space
  def plot_energy_surface(model, ax, title):
      x = np.linspace(-2, 2, 100)
      y = np.linspace(-2, 2, 100)
      xx, yy = np.meshgrid(x, y)
      grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)

      with torch.no_grad():
          zz = model(grid_points).numpy().reshape(xx.shape)

      ax.contourf(xx, yy, zz, levels=50, cmap='viridis')
      ax.scatter([1], [1], c='red', marker='*', s=200, label='Correct Point (1,1)')
      ax.set_title(title)
      ax.legend()"
  language="Python"
/>

<CodeBlock
  code="#let's start training with bad loss function
  model_bad = EnergyModel()
  optimizer_bad = optim.Adam(model_bad.parameters(), lr=0.01)

  #let's iterate this i think along 1000 points?
  print(&#x22;Training with bad loss function&#x22;)
  for epoch in range(1000):
    optimizer_bad.zero_grad()
    energy_correct = model_bad(correct_point) #we just need to pass the correct point to our model,and train based on correct point
    loss_bad = torch.abs(energy_correct)
    loss_bad.backward()
    optimizer_bad.step()

    if epoch % 200 == 0:
      print(f&#x22;Epoch {epoch}, Loss: {loss_bad.item()}&#x22;)


  #let's visualize the result
  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
  plot_energy_surface(model_bad, ax1, &#x22;Collapsed Landscape (Bad Loss)&#x22;)"
  language="Python"
/>

![](/Screenshot%202025-11-23%20at%208.14.54%E2%80%AFAM.png)

<CodeBlock
  code="# --- Training with the STABLE Contrastive Loss ---
  model_stable = EnergyModel() # Use the new class with Tanh
  optimizer_stable = optim.Adam(model_stable.parameters(), lr=0.01)

  print(&#x22;\nTraining with the stable contrastive loss function...&#x22;)
  for epoch in range(2000): # 2000 epochs is usually enough
      optimizer_stable.zero_grad()

      # Push down on the energy of the correct point (towards -1)
      energy_correct = model_stable(correct_point)

      # Pull up on the energy of incorrect, &#x22;negative&#x22; points (towards +1)
      incorrect_points = torch.randn(32, 2) # More points can help
      energy_incorrect = torch.mean(model_stable(incorrect_points))

      # The new, stable loss function
      loss = energy_correct - energy_incorrect

      loss.backward()
      optimizer_stable.step()

      if epoch % 200 == 0:
          print(f&#x22;Epoch {epoch}, Loss: {loss.item():.4f}&#x22;)

  # Visualize the final result
  fig, ax = plt.subplots(figsize=(6, 5))
  plot_energy_surface(model_stable, ax, &#x22;Stable Landscape (Bounded Energy)&#x22;)
  plt.show()"
  language="Python"
/>
