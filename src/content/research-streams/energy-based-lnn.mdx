---
title: >-
  Energy Based Liquid Neural Network: Utilizing Inherent Causality for
  Physics-Constrained Multi-Objective Environment
slug: energy-based-lnn
date: 2025-10-28T00:00:00.000Z
author: Abu Huzaifah bin Haji Bidin
supervisor: 'Dr Nurul Aida Osman (Supervisor), Dr Shuhaida M Shuhidan (Co-supervisor)'
status: active
abstract: >-
  Liquid Neural Networks (LNNs) are a class of Neural Ordinary Differential
  Equations (Neural-ODEs) introduced around 2021. They are designed to model
  continuous-time dynamic systems by replacing the discrete-time hidden state
  updates of Recurrent Neural Networks (RNNs) with a system of continuous-time
  derivatives. LNNs have demonstrated high effectiveness in handling the
  dynamics of control systems and, at the same time, are robust enough to adapt
  to new data regimes, making them highly liquid and flexible for complex
  control environments. A key feature of their architecture is an inherent
  causality, which has shown promise for implementation in robotics and control.
  However, this inherent causality has not been fully explored or analyzed,
  resulting in a research gap. This work will examine and analyze the inherent
  causality in LNNs by applying them in a physics-constrained, multi-objective
  control environment. Utilizing an Energy-Based Model (EBM) as a training
  strategy, it is hoped that the causal pathways of the LNN can be analyzed and
  give insights into the network's decision-making process. It is hypothesized
  that by enforcing physics constraints via an energy-based framework, the LNN's
  learned causal pathways will more closely align with the true physical
  dynamics of the system. This alignment is critical for developing a new class
  of verifiable and interpretable controllers for safety-critical applications.
tags:
  - Liquid Neural Networks
  - Energy Based Models
  - Physics-Informed Neural Networks
  - Advanced Process Control
  - Multi-Objective Optimization
keywords:
  - Liquid neural network
  - Neural-ODEs
  - Energy based models
  - Multi-objective control
  - Physics informed neural network
  - Advanced process control
cover_image: /coverimage-1.png
---

Beware of the non-formal writing here, ramblings has always been my traits

The motivation behind this research is quite simple actually. I have always been fascinated with Liquid Neural Network. Since it appears regularly in my YouTube timeline few years ago, I've been awestruck by the concept.  Listening to Ramin Hasani's Ted Talk, being excited when he present his research in simple terms, makes me truly motivated. The idea presented by Hasani is quite simple. Our brain is plastic. Our brain is not static. Our brain is dynamic, liquid. Therefore the representation of the neural network that we used should be liquid. Not static as it is now that has caused massive overparameterization issues that requires us to spend trillions of dollars on GPUs and data center.

In my mind during that time is one question, “Why hasn't anyone do this?!”. Then I start doing my own digging. I start reading Reddit comments, weighing on the contrastive views from the forum. There's even a harsh question in Reddit, where one of the commenter asking, “Does it really work?” I start doing heavy research. Read the paper, replicate it. Do my own experiments.

Yeah. After that I got my answers. The reason why people aren't doing this is because it's hard to be proven at scale. It does work. You can replicate the paper, and observe exciting results. But does it do any better than current transformers that we have now? No, it does not. Therefore, that's why people are not interested to pursue it that much. There's still some issues with the network that hidden behind Hasani's beautiful Ted Talk. The ugly truth.

Did all of this deter me? Nope. I think what's most interesting about the findings is the journey of it. That's the beauty of research. I think since my late 20s I have always been fascinated by the words of Ibn Al Haytham, the founder of scientific method.

> The seeker after the truth… is not he\* who studies the writings of the ancients and… puts his trust in them, but rather the one who suspects his faith in them and questions what he gathers from them, the one who submits to argument and demonstration, and not to the sayings of a human being whose nature is fraught with all kinds of imperfection and deficiency. It is thus the duty of the man who studies the writings of scientists, if learning the truth is his goal, to make himself an enemy of all that he reads, and, applying his mind to the core and margins of its content, attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency. If he follows this path, the truths will be revealed to him, and whatever shortcomings or uncertainties may exist in the discourse of those who came before him will become manifest.

What Haytham said is actually the spirit of The Golden Age of Islamic Civilization. The spirit that is now lost among the Islamic community. We put trust on the people based on their stature rather than relied on our critical thinking. So that's what I'm trying to emulate. The spirit of scientific method that Al-Haytham has laid down. So, I'll stick with what Haytham has said. If my findings has shown that Liquid Neural Network does not work at scale, I must find ways to show that I am wrong. I must be at war with my own findings. I must make it work at scale. This is where the Energy comes in.

Energy Based Learning is a concept of AI that is being presented by Yann LeCunn. It's actually becoming more mainstream when the original Energy Based Models (EBMs) are recognized by Nobel Prize committee. Both John Hopfield and Geoffrey Hinton are awarded Nobel Prize in Physics due to their work in Hopfield Network and Boltzmann Machine, which are progenitor to the EBMs. The main idea is that instead of deterministic data training, we imagine data as configuration of energy. Good data configuration is low energy while poor data configuration is high energy. Doing it like this during training will create an energy landscape which will form valley at good data configuration, and mountains at the poor data configuration. When we have a landscape that is govern by physics based energy concept, we can use physics law to traverse this landscape (theoretically). That will create an interpretable model that we will be able to control.

So that's the main idea of my research. I want to improve Liquid Neural Networks by combining them with Energy-Based training. By doing this, I aim to create a hybrid model that can be interpreted and controlled. A necessary first step to improving the architecture as a whole.

This blog will serve as the living archive of that journey. It will house all my experiments, theories, and articles on this research, which I have been pursuing since mid-2025.

I hope you find inspiration in this work. Whether you are a fellow researcher, a student, or just someone asking 'Why hasn't anyone done this yet?'. I invite you to join me in suspecting the ancients, questioning the findings, and traversing this energy landscape together. Let's see what truths we can reveal
